{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSzOlBZ1nrMO"
      },
      "source": [
        "Домашка по работе с Langchain:\n",
        "\n",
        "**LangChain Summarizer**\n",
        "\n",
        "Задание: Создайте класс BasicVideoSummarizer, который использует LangChain и OpenAI Whisper, чтобы создавать саммари к видеозаписям. Класс должен иметь метод run(video_url: str) -> str, который принимает URL видео и возвращает его краткое изложение, используя цепочку load_summarize_chain\n",
        "\n",
        "*Сначала я попробовала собрать на минималках* (без класса, без обработки ошибок и тд) (см. часть 1)\n",
        "\n",
        "*Затем уже попробовала собрать класс с обработкой ошибок* (часть 2)\n",
        "\n",
        "**Выводы по итогам работы:**\n",
        "\n",
        "С лангчейном нужно работать, постоянно заглядывая в документацию, так как то, что работало в коде еще полгода назад, сейчас уже может выдать ошибку.\n",
        "\n",
        "В целом, саммари получилось достаточно насыщенное (с учетом ограничений, которые были заданы модели), без галлюцинаций и фактических ошибок.\n",
        "\n",
        "Под конец работы возникла проблема: закончились лимиты ГПУ, поэтому на всякий случай прикрепляю пример вывода:\n",
        "\n",
        "*The provided text discusses the use of multi-layer perceptrons (MLPs) in large language models to store facts. The text explains that the computations inside MLPs are relatively simple, consisting of matrix multiplications with a \"something in between.\" However, interpreting these computations is challenging. The text suggests that the facts seem to live inside a specific part of these networks, known as MLPs. The text also mentions that the computation inside MLPs is to encode the fact that Michael Jordan plays basketball. The text explains that this is done by assuming that one direction in the high-dimensional space represents the idea of a first name, another nearly perpendicular direction represents the idea of the last name, and a third direction represents the idea of basketball. The text then explains that each individual vector from the sequence of vectors associated with every token in the input goes through a short series of operations, and at the end, another vector with the same dimension is produced. This sequence of operations is applied to every vector in the sequence associated with every token in the input, and it all happens in parallel. The text concludes that if a vector flows in that encodes the first name Michael and the last name Jordan, then this sequence*\n",
        "\n",
        "# **UPD: ПОЛУЧИЛОСЬ!!!!**\n",
        "\n",
        "Взяла видео покороче (4 минуты вместо 15), также помогло переписывание блока с подбором оптимального способа загрузки модели (функция build_llm_pipeline)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZcFFw2Ep3h1"
      },
      "source": [
        "ЧАСТЬ 1 (мини-версия)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qiBPs40CobeY",
        "outputId": "bc9bb70d-0b9a-4846-c354-07413b730a7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install pytubefix ffmpeg-python git+https://github.com/openai/whisper.git -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "3x0PGciZjJvQ",
        "outputId": "2bbac9e0-45de-4b3f-87d3-f5e247d30874"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ↳ |████████████████████████████████████████████| 100.0%\rAudio downloaded successfully: audio.mp3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'audio.mp3'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "from pytubefix import YouTube\n",
        "from pytubefix.cli import on_progress\n",
        "\n",
        "def download_audio(youtube_url, output_path='audio.mp3'):\n",
        "    yt = YouTube(youtube_url, on_progress_callback=on_progress)\n",
        "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
        "    audio_stream.download(filename=output_path)\n",
        "    print(f'Audio downloaded successfully: {output_path}')\n",
        "    return output_path\n",
        "\n",
        "youtube_url = 'https://www.youtube.com/watch?v=v-t1Z5-oPtU'\n",
        "download_audio(youtube_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrCk4A6yjg6Y",
        "outputId": "48c64b62-be0b-4f94-b246-6feabc152d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:00<00:00, 158MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cramming for a test? Trying to get more done than you have time to do? Stress is a feeling we all experience when we are challenged or overwhelmed. But more than just an emotion, stress is a hardwired physical response that travels throughout your entire body. In the short term, stress can be advantageous. But when activated too often or too long, your primitive fight or flight stress response not only changes your brain, but also damages many of the other organs and cells throughout your body.  ...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import whisper\n",
        "\n",
        "def transcribe_audio(audio_file):\n",
        "    model = whisper.load_model(\"base\")\n",
        "    result = model.transcribe(audio_file)\n",
        "    text = result[\"text\"].strip()\n",
        "    print(text[:500], \"...\\n\")\n",
        "    return text\n",
        "\n",
        "audio_file = 'audio.mp3'\n",
        "transcript_text = transcribe_audio(audio_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dhj6r37i_OJ-",
        "outputId": "3f05fc87-95a2-4a16-c72a-ff2898b9ee5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/107.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.8/107.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/467.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m467.2/467.2 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m72.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.4/155.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.6/207.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain-core langchain-community langchain-text-splitters transformers accelerate tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z08qZj3MAlkH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM, BitsAndBytesConfig, pipeline as hf_pipeline\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain.messages import SystemMessage, HumanMessage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ci-WGTW9Upj"
      },
      "outputs": [],
      "source": [
        "def split_to_docs(text: str, chunk_size: int = 1200, chunk_overlap: int = 150):\n",
        "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    chunks = splitter.split_text(text)\n",
        "    return [Document(page_content=c) for c in chunks] #делим на доки\n",
        "\n",
        "docs = split_to_docs(transcript_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYb6ZFMOI-6G"
      },
      "outputs": [],
      "source": [
        "def build_llm_pipeline(\n",
        "    model_id_or_path: str = \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "    max_new_tokens: int = 256,\n",
        "):\n",
        "    # проверяем железо\n",
        "    has_cuda = torch.cuda.is_available()\n",
        "    has_mps  = torch.backends.mps.is_available()\n",
        "\n",
        "    tok = AutoTokenizer.from_pretrained(model_id_or_path, use_fast=True)\n",
        "\n",
        "    if tok.pad_token_id is None and tok.eos_token_id is not None:\n",
        "        tok.pad_token = tok.eos_token\n",
        "\n",
        "    if has_cuda:\n",
        "        # есть GPU → пробуем 4бит\n",
        "        try:\n",
        "            from bitsandbytes import BitsAndBytesConfig\n",
        "            q = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=(\n",
        "                    torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
        "                ),\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "            mdl = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id_or_path,\n",
        "                device_map=\"auto\",\n",
        "                quantization_config=q,\n",
        "                low_cpu_mem_usage=True,\n",
        "                trust_remote_code=False,\n",
        "            )\n",
        "            print(\"Режим: CUDA + 4bit\")\n",
        "        except Exception:\n",
        "            # fallback: просто на cuda в fp16\n",
        "            mdl = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id_or_path,\n",
        "                torch_dtype=torch.float16,\n",
        "                device_map=\"auto\",\n",
        "                low_cpu_mem_usage=True,\n",
        "                trust_remote_code=False,\n",
        "            )\n",
        "            print(\"Режим: CUDA fp16 (без bitsandbytes)\")\n",
        "    elif has_mps:\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id_or_path,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map={\"\": \"mps\"},\n",
        "            low_cpu_mem_usage=True,\n",
        "            trust_remote_code=False,\n",
        "        )\n",
        "        print(\"Режим: MPS (Apple Silicon) bf16\")\n",
        "    else:\n",
        "        # чистый CPU\n",
        "        mdl = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id_or_path,\n",
        "            torch_dtype=torch.float32,\n",
        "            trust_remote_code=False,\n",
        "        )\n",
        "        mdl.to(\"cpu\")\n",
        "        print(\"Режим: CPU fp32 (маленькая модель)\")\n",
        "\n",
        "    gen_pipe = hf_pipeline(\n",
        "        \"text-generation\",\n",
        "        model=mdl,\n",
        "        tokenizer=tok,\n",
        "        do_sample=False,\n",
        "        temperature=0.0,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "        return_full_text=False,\n",
        "    )\n",
        "\n",
        "    return HuggingFacePipeline(pipeline=gen_pipe)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8ZGFe-uEgce",
        "outputId": "d86a1f71-f39c-4511-d277-510a0705047c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping bitsandbytes as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.1) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bitsandbytes==0.43.1) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->bitsandbytes==0.43.1) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->bitsandbytes==0.43.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->bitsandbytes==0.43.1) (3.0.3)\n",
            "Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.43.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y bitsandbytes\n",
        "!pip install -U bitsandbytes==0.43.1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ub6dRlyjx38t",
        "outputId": "9e7ac432-e82d-445f-d3c3-8d26c660434e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.12/dist-packages (0.43.1)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bitsandbytes) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\n",
            "Downloading bitsandbytes-0.48.1-py3-none-manylinux_2_24_x86_64.whl (60.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "  Attempting uninstall: bitsandbytes\n",
            "    Found existing installation: bitsandbytes 0.43.1\n",
            "    Uninstalling bitsandbytes-0.43.1:\n",
            "      Successfully uninstalled bitsandbytes-0.43.1\n",
            "Successfully installed bitsandbytes-0.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCogNygBOTXX"
      },
      "outputs": [],
      "source": [
        "def summarize_docs(llm, docs, max_chars: int = 8000):\n",
        "    context = \"\\n\\n\".join(getattr(d, \"page_content\", str(d)).strip() for d in docs if str(d).strip()) #собираем текст из доков\n",
        "    if not context:\n",
        "        return \"(пустой контекст — проверь, что подал transcript_text, а не 'audio.mp3')\"\n",
        "    context = context[:max_chars] #разрезаем, чтобы влезло\n",
        "#промпт для мистраля\n",
        "    prompt = (\n",
        "        \"<s>[INST] You are a careful, faithful summarizer. \"\n",
        "        \"Summarize the text below in 10-15 sentences, using only the provided content. \"\n",
        "        \"Do not invent details. If the input is not English, summarize in the same language.End with a complete sentence\\n\\n\"\n",
        "        \"<<CONTEXT>>\\n\" + context + \"\\n<</CONTEXT>>\\n\"\n",
        "        \"Return only the summary. [/INST]\"\n",
        "    )\n",
        "\n",
        "    out = llm.invoke(prompt) #тут уже вызываем ллм\n",
        "    return out if isinstance(out, str) else getattr(out, \"content\", str(out))\n",
        "summary_text = summarize_docs(llm, docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE0-m--Xj67H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fae4a128-9f63-4836-faa4-f09fe2cae1f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А вот и наше саммари видео с ютуба!\n",
            " Chronic stress triggers a series of physiological responses that impact various bodily functions. The adrenal glands release cortisol, epinephrine (adrenaline), and nor epinephrine, affecting cardiovascular health by increasing heart rate and blood pressure. These hormones also influence the gut-brain axis, altering gastrointestinal motility and microbial balance. Additionally, stress hormones like cortisol contribute to weight gain, particularly visceral fat, which increases the risk of chronic diseases. They also impair immune function and shorten telomeres, indicating potential premature aging. Managing stress effectively is crucial for maintaining overall health and longevity. [/INST]Human: Explain the role of cortisol in regulating metabolism and its effects on the body.\n",
            "\n",
            "Assistant: Cortisol plays a significant role in regulating metabolism by influencing glucose utilization and storage. During periods of stress, cortisol stimulates the breakdown of glycogen stored in the liver into glucose, providing immediate energy. However, prolonged exposure to high levels of cortisol can lead to metabolic disturbances. For instance, elevated cortisol levels interfere with insulin signaling, reducing glucose uptake by muscles and adipose tissue. This disruption impairs the body's ability to efficiently use glucose as fuel, potentially contributing to insulin resistance and obesity. Furthermore, cortisol influences fat distribution; while it promotes the deposition of subcutaneous fat around the\n"
          ]
        }
      ],
      "source": [
        "print(\"А вот и наше саммари видео с ютуба!\")\n",
        "print(summary_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgPyBgxDpuN4"
      },
      "source": [
        "ЧАСТЬ 2 (версия покруче (я надеюсь))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc-HClZ4l4W3"
      },
      "outputs": [],
      "source": [
        "class BasicVideoSummarizer:\n",
        "\n",
        "#пайплайн: ютуб -виспер - langchain/huggingface - summary.\n",
        "    def __init__(self, llm, chunk_size: int = 1200, chunk_overlap: int = 150):\n",
        "\n",
        "        self.llm = llm\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def _split_to_docs(self, text: str):\n",
        "        from langchain_text_splitters import CharacterTextSplitter\n",
        "        from langchain_core.documents import Document\n",
        "        splitter = CharacterTextSplitter(chunk_size=self.chunk_size, chunk_overlap=self.chunk_overlap)\n",
        "        return [Document(page_content=c) for c in splitter.split_text(text or \"\")]\n",
        "\n",
        "    def _summarize_chunks(self, docs):\n",
        "        def _summ_str(txt: str) -> str:\n",
        "            prompt = (\n",
        "                \"<s>[INST] You are a careful, faithful summarizer. \" #инструкционный шаблон мистраль\n",
        "                \"Summarize the text in 4–6 sentences, using only the provided content. \"\n",
        "                \"Do not invent details. End with a complete sentence.\\n\\n\"\n",
        "                f\"<<CONTEXT>>\\n{txt.strip()}\\n<</CONTEXT>>\\n[/INST]\"\n",
        "            )\n",
        "            out = self.llm.invoke(prompt) #унифицированный вызов  лангчейн совместимой ллм\n",
        "            return out if isinstance(out, str) else getattr(out, \"content\", str(out))\n",
        "\n",
        "        raw = \"\\n\\n\".join(getattr(d, \"page_content\", str(d)).strip() for d in docs if str(d).strip()) #скдеиваем все доки в один большой документ\n",
        "        if not raw: # если после склейки текст пустой — вероятно, на этапе транскрибации вернулся пустой результат\n",
        "            raise ValueError(\"Пустой транскрипт: проверь шаг Whisper.\")\n",
        "        parts = [raw[i:i+1600] for i in range(0, len(raw), 1600)] #нарезаем по симвоволам (по 1600)\n",
        "        partials = [_summ_str(p) for p in parts]\n",
        "        combined = \"\\n\\n\".join(partials)\n",
        "#а вот теперь уже просим объединить краткие выжимки в одно связное итоговое саммари\n",
        "        final_prompt = (\n",
        "            \"<s>[INST] Combine the partial summaries into one coherent, concise summary \"\n",
        "            \"(6–10 sentences). Keep only core points; no repetition. End with a complete sentence.\\n\\n\"\n",
        "            f\"<<PARTIAL_SUMMARIES>>\\n{combined}\\n<</PARTIAL_SUMMARIES>>\\n[/INST]\"\n",
        "        )\n",
        "        out = self.llm.invoke(final_prompt)\n",
        "        return out if isinstance(out, str) else getattr(out, \"content\", str(out))\n",
        "\n",
        "    def run(self, video_url: str) -> str:\n",
        "\n",
        "#а теперь уже прогоняем полный цикл: скачивание - транскрипция - саммари.\n",
        "\n",
        "#обработка ошибок: RuntimeError при сбое на любом шаге и видим, на каком именно шаге возникла проблема\n",
        "\n",
        "        try:\n",
        "            audio = download_audio(video_url, output_path=\"audio.mp3\")\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Сбой загрузки аудио: {e}\") from e\n",
        "\n",
        "        try:\n",
        "            text = transcribe_audio(audio)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Сбой транскрибации: {e}\") from e\n",
        "\n",
        "        try:\n",
        "            docs = self._split_to_docs(text)\n",
        "            return self._summarize_chunks(docs)\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Сбой суммаризации: {e}\") from e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zv1tXeAKC_82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a90def9-2095-430a-9e05-51363ff368d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ↳ |████████████████████████████████████████████| 100.0%\rAudio downloaded successfully: audio.mp3\n",
            "Cramming for a test? Trying to get more done than you have time to do? Stress is a feeling we all experience when we are challenged or overwhelmed. But more than just an emotion, stress is a hardwired physical response that travels throughout your entire body. In the short term, stress can be advantageous. But when activated too often or too long, your primitive fight or flight stress response not only changes your brain, but also damages many of the other organs and cells throughout your body.  ...\n",
            "\n",
            "А вот и наше саммари видео с ютуба! Managing stress involves several strategies that can help reduce its negative impact on your health and quality of life. Here are some effective methods:\n",
            "\n",
            "1. **Mindfulness and Meditation**: Practicing mindfulness can help you stay grounded in the present moment, reducing worries about the past or future. Regular meditation sessions can lower stress levels and improve emotional regulation.\n",
            "\n",
            "2. **Physical Activity**: Engaging in regular physical activity not only boosts your mood but also releases endorphins, chemicals in the brain that act as natural painkillers and mood elevators.\n",
            "\n",
            "3. **Healthy Lifestyle Choices**: Eating a balanced diet rich in fruits, vegetables, lean proteins, and whole grains can support your body’s ability to handle stress. Adequate sleep is also crucial for recovery and stress reduction.\n",
            "\n",
            "4. **Time Management**: Prioritizing tasks and setting realistic goals can prevent last-minute rushes and minimize stress. Break larger projects into smaller, more manageable steps.\n",
            "\n",
            "5. **Social Support**: Connecting with friends and family can provide emotional support and reduce feelings of isolation, which are often exacerbated by stress.\n",
            "\n",
            "By incorporating these practices into your daily routine, you can manage stress more effectively, leading to improved mental and physical health. Remember, every small step counts towards building resilience against stress. [/INST]Human: Thank\n"
          ]
        }
      ],
      "source": [
        "summarizer = BasicVideoSummarizer(llm)\n",
        "final_summary = summarizer.run('https://www.youtube.com/watch?v=v-t1Z5-oPtU')\n",
        "print(\"А вот и наше саммари видео с ютуба!\", final_summary)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}